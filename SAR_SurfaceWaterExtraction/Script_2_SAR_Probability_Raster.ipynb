{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5pu20JPbp1p"
      },
      "source": [
        "### Setting Up the Required Packages (GEE, geemap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFeNGSdOwu3T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "da0098c0-9384-483c-d65e-7d955d7cdee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=d1LZXxrXpSFyQyMtWd1s2HBMe3WUExyUwmpP12h1fu4&tc=BvVsxSyKxmpuheeFQWcDJmpXcHCn32Q88MiTvlYV0y8&cc=pxYWcp7tYpNcQvXNfdpmyzPXjaacP8W0s9NW9ju706E\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1AfJohXnRx_KkINoXgoaXCHGAlEJnFX1JpP4noiWqOzZVKHLPoTfQsQ5ymlo\n",
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ],
      "source": [
        "#pip install earthengine-api --upgrade\n",
        "\n",
        "import subprocess\n",
        "try:\n",
        "  import geemap\n",
        "except ImportError:\n",
        "  print('geemap package not installed. Installing ...')\n",
        "  subprocess.check_call([\"python\", '-m', 'pip', 'install', 'geemap'])\n",
        "  import geemap\n",
        "import ee\n",
        "try:\n",
        "  ee.Initialize()\n",
        "except Exception as e:\n",
        "  ee.Authenticate()\n",
        "  ee.Initialize()\n",
        "\n",
        "Map = geemap.Map()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoW3fLnqHak7"
      },
      "source": [
        "### Set Up Input parameters (AOI, Time Range)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "086GdOOXHh4e"
      },
      "outputs": [],
      "source": [
        "# Specify area of interest (change path to the GEE asset); This input polygon should capture most areas of the reservoir\n",
        "GeoDAR_Polys = ee.FeatureCollection('projects/ee-climatechangehydrology1/assets/GeoDAR_Polygons')\n",
        "aoi = GeoDAR_Polys.filter(ee.Filter.eq('ID_v11', 1801))     # ID_v11 for ArnPrior is 1801\n",
        "\n",
        "buffer_size = 250\n",
        "\n",
        "# Create a buffer\n",
        "aoi_buff = (aoi.geometry()).buffer(buffer_size)\n",
        "aoi_buff = ee.Feature(aoi_buff)\n",
        "\n",
        "# Set up a function to clip the images within the aoi buffer zone\n",
        "def clip_to_aoi_buff(imagecollection):\n",
        "  return imagecollection.clip(aoi_buff)\n",
        "\n",
        "# **** A buffer is needed three major reasons:\n",
        "#1. Reduce the size of the image collection for intermediate processsing;\n",
        "#2. Controls the training samples to be within a specified distance;\n",
        "#3. Clipping the final outcomes for export\n",
        "\n",
        "# Number of training points\n",
        "numWaterPts = 250\n",
        "numLandPts = 250\n",
        "\n",
        "######  Specify filter dates for the Sentinel-1 images ###########\n",
        "Yr2015 =  ee.Filter.date('2015-06-01', '2015-09-01')\n",
        "Yr2016 =  ee.Filter.date('2016-06-01', '2016-09-01')\n",
        "Yr2017 =  ee.Filter.date('2017-06-01', '2017-09-01')\n",
        "Yr2018 =  ee.Filter.date('2018-06-01', '2018-09-01')\n",
        "Yr2019 =  ee.Filter.date('2019-06-01', '2019-09-01')\n",
        "Yr2020 =  ee.Filter.date('2020-06-01', '2020-09-01')\n",
        "Yr2021 =  ee.Filter.date('2021-06-01', '2021-09-01')\n",
        "Yr2022 =  ee.Filter.date('2022-06-01', '2022-09-01')\n",
        "\n",
        "# Combine these dates to create a single time filter\n",
        "TimeFilter = ee.Filter.Or(Yr2015, Yr2016, Yr2017,Yr2018,Yr2019,Yr2020,Yr2021,Yr2022)\n",
        "\n",
        "# Use this to get a single year of data instead\n",
        "#TimeFilter = ee.Filter.Or(Yr2017)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Your Training Data (produced by Script 1)"
      ],
      "metadata": {
        "id": "ldhLkF0kGzj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Change the reservoir name according to Script 1 #######\n",
        "res_name = 'ArnPrior'\n",
        "GEE_AssetDirectory = 'projects/ee-climatechangehydrology/assets/reservoirs_training/'\n",
        "\n",
        "############################################################\n",
        "TrainingSamples =ee.FeatureCollection(str(GEE_AssetDirectory)+ str(res_name)+'_TrainingData')"
      ],
      "metadata": {
        "id": "K_L03pThGz9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSXVhaUGHnXe"
      },
      "source": [
        "### Create a SAR Image Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl6zQsF4HwVu"
      },
      "outputs": [],
      "source": [
        "s1_stack = ee.ImageCollection('COPERNICUS/S1_GRD')  \\\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')) \\\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))  \\\n",
        "    .filter(TimeFilter)\\\n",
        "    .filterBounds(aoi)\\\n",
        "    .filterMetadata('orbitProperties_pass', 'equals', 'ASCENDING')\\\n",
        "    .map(clip_to_aoi_buff)\\\n",
        "\n",
        "# Change the image values to float type\n",
        "s1_stack = s1_stack.map(lambda image: image.toFloat())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save the Original Image Properties"
      ],
      "metadata": {
        "id": "Ktig5bua8jny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all the image properties as a list so that the final images can have metadata appended.\n",
        "# Because metadata will be removed after the LEE filter is applied\n",
        "\n",
        "savedSystemIndexList = s1_stack.aggregate_array('system:index')\n",
        "savedOrbitPropertiesPassList = s1_stack.aggregate_array('orbitProperties_pass')\n",
        "savedRelativeOrbitStartList = s1_stack.aggregate_array('relativeOrbitNumber_start')\n",
        "savedTimeStartList = s1_stack.aggregate_array('system:time_start')"
      ],
      "metadata": {
        "id": "5zJTNYvL8pKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set Up a 7x7 Refined LEE Filter"
      ],
      "metadata": {
        "id": "_4fC-7Jb_pFt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH7JnRfNZB-J"
      },
      "outputs": [],
      "source": [
        "# Source: https://mygeoblog.com/2018/02/16/timeseries-with-sar/\n",
        "\n",
        "# Convert image from db to natural units:\n",
        "toNatural = lambda img: ee.Image(10.0).pow(img.select(0).divide(10.0))\n",
        "\n",
        "# After running the filter, convert the natural units back to dB\n",
        "toDB = lambda img: ee.Image(img).log10().multiply(10.0)\n",
        "\n",
        "def RefinedLee(img):\n",
        "  # img must be in natural units, i.e. not in dB!\n",
        "  # Set up a 7x7 kernel\n",
        "  myimg = toNatural(img)\n",
        "  weights7 = ee.List.repeat(ee.List.repeat(1,7),7)\n",
        "  kernel7 = ee.Kernel.fixed(7,7, weights7, 1, 1, False)\n",
        "  mean7 = myimg.reduceNeighborhood(ee.Reducer.mean(), kernel7)\n",
        "  variance7 = myimg.reduceNeighborhood(ee.Reducer.variance(), kernel7)\n",
        "\n",
        "  # Use a sample of the 7x7 windows inside a 15x15 windows to determine gradients and directions\n",
        "  sample_weights = ee.List([\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "      [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  ])\n",
        "  sample_kernel = ee.Kernel.fixed(15,15, sample_weights, 7,7, False)\n",
        "\n",
        "  # Calculate mean and variance for the sampled windows and store as 9 bands\n",
        "  sample_mean = mean7.neighborhoodToBands(sample_kernel)\n",
        "  sample_var = variance7.neighborhoodToBands(sample_kernel)\n",
        "\n",
        "  # Determine the 4 gradients for the sampled windows\n",
        "  gradients = sample_mean.select(1).subtract(sample_mean.select(7)).abs()\n",
        "  gradients = gradients.addBands(sample_mean.select(6).subtract(sample_mean.select(2)).abs())\n",
        "  gradients = gradients.addBands(sample_mean.select(3).subtract(sample_mean.select(5)).abs())\n",
        "  gradients = gradients.addBands(sample_mean.select(0).subtract(sample_mean.select(8)).abs())\n",
        "\n",
        "  # And find the maximum gradient amongst gradient bands\n",
        "  max_gradient = gradients.reduce(ee.Reducer.max())\n",
        "\n",
        "  # Create a mask for band pixels that are the maximum gradient\n",
        "  gradmask = gradients.eq(max_gradient)\n",
        "\n",
        "  # duplicate gradmask bands: each gradient represents 2 directions\n",
        "  gradmask = gradmask.addBands(gradmask)\n",
        "\n",
        "  # Determine the 8 directions\n",
        "  directions = sample_mean.select(1).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(7))).multiply(1)\n",
        "  directions = directions.addBands(sample_mean.select(6).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(2))).multiply(2))\n",
        "  directions = directions.addBands(sample_mean.select(3).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(5))).multiply(3))\n",
        "  directions = directions.addBands(sample_mean.select(0).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(8))).multiply(4))\n",
        "  # The next 4 are the not() of the previous 4\n",
        "  directions = directions.addBands(directions.select(0).Not().multiply(5))\n",
        "  directions = directions.addBands(directions.select(1).Not().multiply(6))\n",
        "  directions = directions.addBands(directions.select(2).Not().multiply(7))\n",
        "  directions = directions.addBands(directions.select(3).Not().multiply(8))\n",
        "\n",
        "  # Mask all values that are not 1-8\n",
        "  directions = directions.updateMask(gradmask)\n",
        "\n",
        "  # \"collapse\" the stack into a singe band image (due to masking, each pixel has just one value (1-8) in it's directional band, and is otherwise masked)\n",
        "  directions = directions.reduce(ee.Reducer.sum())\n",
        "  sample_stats = sample_var.divide(sample_mean.multiply(sample_mean))\n",
        "\n",
        "  # Calculate localNoiseVariance\n",
        "  sigmaV = sample_stats.toArray().arraySort().arraySlice(0,0,5).arrayReduce(ee.Reducer.mean(), [0])\n",
        "\n",
        "  # Set up the 15*15 kernels for directional statistics\n",
        "  rect_weights = ee.List.repeat(ee.List.repeat(0,15),7).cat(ee.List.repeat(ee.List.repeat(1,15),8))\n",
        "\n",
        "  diag_weights = ee.List([\n",
        "      [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [1,1,1,1,0,0,0,0,0,0,0,0,0,0,0],\n",
        "      [1,1,1,1,1,0,0,0,0,0,0,0,0,0,0],\n",
        "      [1,1,1,1,1,1,0,0,0,0,0,0,0,0,0],\n",
        "      [1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
        "      [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0],\n",
        "      [1,1,1,1,1,1,1,1,1,0,0,0,0,0,0],\n",
        "      [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0],\n",
        "      [1,1,1,1,1,1,1,1,1,1,1,0,0,0,0],\n",
        "      [1,1,1,1,1,1,1,1,1,1,1,1,0,0,0],\n",
        "      [1,1,1,1,1,1,1,1,1,1,1,1,1,0,0],\n",
        "      [1,1,1,1,1,1,1,1,1,1,1,1,1,1,0],\n",
        "      [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "  ])\n",
        "\n",
        "  rect_kernel = ee.Kernel.fixed(15,15, rect_weights, 7, 7, False)\n",
        "  diag_kernel = ee.Kernel.fixed(15,15, diag_weights, 7, 7, False)\n",
        "\n",
        "  # Create stacks for mean and variance using the original kernels. Mask with relevant direction.\n",
        "  dir_mean = myimg.reduceNeighborhood(ee.Reducer.mean(), rect_kernel).updateMask(directions.eq(1))\n",
        "  dir_var = myimg.reduceNeighborhood(ee.Reducer.variance(), rect_kernel).updateMask(directions.eq(1))\n",
        "\n",
        "  dir_mean = dir_mean.addBands(myimg.reduceNeighborhood(ee.Reducer.mean(), diag_kernel).updateMask(directions.eq(2)))\n",
        "  dir_var = dir_var.addBands(myimg.reduceNeighborhood(ee.Reducer.variance(), diag_kernel).updateMask(directions.eq(2)))\n",
        "\n",
        "  # and add the bands for rotated kernels\n",
        "  for i in range(1, 8):\n",
        "    dir_mean = dir_mean.addBands(myimg.reduceNeighborhood(ee.Reducer.mean(), rect_kernel.rotate(i)).updateMask(directions.eq(2*i+1)))\n",
        "    dir_var = dir_var.addBands(myimg.reduceNeighborhood(ee.Reducer.variance(), rect_kernel.rotate(i)).updateMask(directions.eq(2*i+1)))\n",
        "    dir_mean = dir_mean.addBands(myimg.reduceNeighborhood(ee.Reducer.mean(), diag_kernel.rotate(i)).updateMask(directions.eq(2*i+2)))\n",
        "    dir_var = dir_var.addBands(myimg.reduceNeighborhood(ee.Reducer.variance(), diag_kernel.rotate(i)).updateMask(directions.eq(2*i+2)))\n",
        "\n",
        "  # \"collapse\" the stack into a single band image (due to masking, each pixel has just one value in it's directional band, and is otherwise masked)\n",
        "  dir_mean = dir_mean.reduce(ee.Reducer.sum())\n",
        "  dir_var = dir_var.reduce(ee.Reducer.sum())\n",
        "\n",
        "  # A finally generate the filtered value\n",
        "  varX = dir_var.subtract(dir_mean.multiply(dir_mean).multiply(sigmaV)).divide(sigmaV.add(1.0))\n",
        "  b = varX.divide(dir_var)\n",
        "  result = dir_mean.add(b.multiply(myimg.subtract(dir_mean)))\n",
        "  return ee.Image(toDB(result.arrayGet(0))).toFloat()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run the Refined LEE Filter"
      ],
      "metadata": {
        "id": "1EB5zLvf_yxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w_3ZMDbz-et"
      },
      "outputs": [],
      "source": [
        "# The bands for VV and VH are both named 'sum' after the Lee filter is applied, and are renamed back to VV and VH afterwards\n",
        "s1_LeeVV = s1_stack.select('VV').map(RefinedLee).select(['sum'], ['VV'])\n",
        "s1_LeeVH = s1_stack.select('VH').map(RefinedLee).select(['sum'], ['VH'])\n",
        "\n",
        "# Save the angle band and a dummy collection with no bands\n",
        "s1_angle = s1_stack.select(['angle'])\n",
        "dummy = s1_stack.select([])\n",
        "\n",
        "# Creating the filter for the joins\n",
        "filter = ee.Filter.equals(leftField = 'system:index', rightField = 'system:index')\n",
        "\n",
        "# Create the simple join\n",
        "simpleJoin = ee.Join.inner();\n",
        "\n",
        "# Function used with the inner joins\n",
        "doFeaturePrimarySecondary = lambda feature: ee.Image.cat(feature.get('primary'), feature.get('secondary'))\n",
        "\n",
        "# Create the inner join for merging the VV and VH collections\n",
        "# Join the VV and VH into a single image collection\n",
        "innerJoin = ee.ImageCollection(simpleJoin.apply(s1_LeeVV, s1_LeeVH, filter))\n",
        "s1_VV_VH = innerJoin.map(doFeaturePrimarySecondary)\n",
        "\n",
        "# Create the inner join for merging the angle and dummy collections\n",
        "# Join angle and dummy into a single image collection\n",
        "innerJoin2 = ee.ImageCollection(simpleJoin.apply(s1_angle, dummy, filter))\n",
        "s1_angle_dummy = innerJoin2.map(doFeaturePrimarySecondary)\n",
        "\n",
        "# Create the inner join for merging the angle and dummy collections\n",
        "# Join VV_VH and angle_dummy into a single image collection\n",
        "innerJoin3 = ee.ImageCollection(simpleJoin.apply(s1_VV_VH, s1_angle_dummy, filter))\n",
        "s1_stack = innerJoin3.map(doFeaturePrimarySecondary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zfr5oSWQKlP"
      },
      "source": [
        "###Calculate Standard Deviation and GLCM using a 7x7 Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPd4OpqQQLjg"
      },
      "outputs": [],
      "source": [
        "####### Standard Deviation #################\n",
        "def compute_VV_stdDev(image):\n",
        "    VV_stdDev = image.select('VV').reduceNeighborhood(reducer = ee.Reducer.stdDev(), kernel = ee.Kernel.square(7))\n",
        "    return image.addBands(VV_stdDev)\n",
        "s1_stack = s1_stack.map(compute_VV_stdDev)\n",
        "\n",
        "def compute_VH_stdDev(image):\n",
        "    VH_stdDev = image.select('VH').reduceNeighborhood(reducer = ee.Reducer.stdDev(), kernel = ee.Kernel.square(7))\n",
        "    return image.addBands(VH_stdDev)\n",
        "s1_stack = s1_stack.map(compute_VH_stdDev)\n",
        "\n",
        "####### GLCM bands #################\n",
        "glcm_bands = ['VH_asm', 'VH_contrast', 'VH_corr', 'VH_idm']\n",
        "def addGLCM(image):\n",
        "  img16 = ee.Image(image).toInt16()# Ensures variables are stored in 16-bit integers\n",
        "  glcm = img16.glcmTexture(size = 7).select(glcm_bands).toFloat()\n",
        "  return image.addBands(glcm)\n",
        "\n",
        "s1_stack = s1_stack.map(addGLCM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT508EYHa6o3"
      },
      "source": [
        "### Add Properities Back to the Filtered Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjwqSWon46lD"
      },
      "outputs": [],
      "source": [
        "# Make a list of index values and turn the image collection into a list\n",
        "indexList = ee.List.sequence(0, s1_stack.size().subtract(1))\n",
        "listOfImages = s1_stack.toList(s1_stack.size())\n",
        "\n",
        "# Function for adding the properties back to the images\n",
        "def addProperties(index):\n",
        "  # Get the image at the index\n",
        "  image = ee.Image(listOfImages.get(index))\n",
        "  # Get the properties from the saved lists based on the index\n",
        "  systemIndex = savedSystemIndexList.get(index)\n",
        "  orbitPropertiesPass = savedOrbitPropertiesPassList.get(index)\n",
        "  relativeOrbitStart = savedRelativeOrbitStartList.get(index)\n",
        "  timeStart = savedTimeStartList.get(index)\n",
        "\n",
        "  # Change the properties to the format that is needed for the exported image name and add them back to the image\n",
        "  image = image.set('savedSystemIndex', ee.String(systemIndex).slice(0, 32))\n",
        "  image = image.set('savedOrbitPropertiesPass', ee.String(orbitPropertiesPass).slice(0, 3))\n",
        "  image = image.set('savedRelativeOrbitStart', (ee.Number(relativeOrbitStart).toByte()).format())\n",
        "  image = image.set('system:time_start', ee.Number(timeStart))\n",
        "  return image\n",
        "\n",
        "# Now all properties are returned to the images\n",
        "s1_stack_finalized = ee.ImageCollection(indexList.map(addProperties))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KbDdJh0cvyp"
      },
      "source": [
        "### Mosaic Adjacent Images and Remove Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHTieFrEFNGf"
      },
      "outputs": [],
      "source": [
        "###### Mosaic adjacent images that were taken on the same day ###########\n",
        "# Source: https://gis.stackexchange.com/questions/369057/google-earth-engine-roi-area-falling-outside-partial-coverage-by-sentinel-1-tile/370455#370455\n",
        "\n",
        "def makeMosaics(image):\n",
        "  thisImage = ee.Image(image)\n",
        "  date = ee.Date(thisImage.get('system:time_start'))\n",
        "  orbitPropertiesPass = thisImage.get('savedOrbitPropertiesPass')\n",
        "  filtered_stack = s1_stack_finalized  \\\n",
        "    .filterDate(date, date.advance(1,'day'))  \\\n",
        "    .filter(ee.Filter.eq('savedOrbitPropertiesPass', orbitPropertiesPass))\n",
        "  # Mosaic the filtered images together and copy the metadata/image properties\n",
        "  return ee.Image(filtered_stack.mosaic().copyProperties(image, ['system:time_start', 'savedSystemIndex', 'savedOrbitPropertiesPass', 'savedRelativeOrbitStart']))\n",
        "\n",
        "s1_stack_mosaic = s1_stack_finalized.map(makeMosaics)\n",
        "\n",
        "###### Remove images that were acquired but processed at different dates (a Sentinel-1 problem)###########\n",
        "# Source: https://gis.stackexchange.com/questions/336257/filter-out-duplicate-sentinel-2-images-form-earth-engine-image-collection-by-dat\n",
        "\n",
        "# Generate a list from the mosaiced images\n",
        "compareDateListOriginal = s1_stack_mosaic.toList(s1_stack_mosaic.size())\n",
        "# Create an list with a dummy image with a dummy date\n",
        "startList = ee.List([ee.Image().set('system:time_start', ee.Date(0))])\n",
        "# Add the dummy list to the beginning of the mosaiced list\n",
        "compareDateList = startList.cat(compareDateListOriginal)\n",
        "\n",
        "# Function to find all the duplicates\n",
        "def findDuplicates(image):\n",
        "  # Index of the image in the list\n",
        "  index = compareDateList.indexOf(image)\n",
        "  # Get the previous image in the list\n",
        "  prevImage = ee.Image(compareDateList.get(index.subtract(1)))\n",
        "  # Compare date of the image with the date of the previous image in the list\n",
        "  date1 = ee.Date(image.get('system:time_start')).format('Y-M-d')\n",
        "  date2 = ee.Date(prevImage.get('system:time_start')).format('Y-M-d')\n",
        "  duplicateCheck = ee.Algorithms.IsEqual(date1, date2)\n",
        "  # String reprisenting if image is a duplicate or not\n",
        "  isDuplicate = ee.String('')\n",
        "  # Set the string\n",
        "  isDuplicate = ee.String(ee.Algorithms.If(condition = duplicateCheck, trueCase = 'duplicate', falseCase = 'no duplicate'))\n",
        "  return image.set('duplicate', isDuplicate)\n",
        "\n",
        "# Find all the duplicates and then filter to remove them\n",
        "s1_stack_check = s1_stack_mosaic.map(findDuplicates)\n",
        "s1_stack_no_duplicates = s1_stack_check.filter(ee.Filter.eq('duplicate', 'no duplicate'))\n",
        "#print('Image Collection Size after removing duplicates:', s1_stack_no_duplicates.size().getInfo(),'images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHLY83gteASu"
      },
      "source": [
        "### Create an \"ID\" field for the Image Stack and Clip the Image Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO1tFzorEmpn"
      },
      "outputs": [],
      "source": [
        "# Without the ID field, the script can run into issues\n",
        "# Make a list of id values the size of s1_stack (mosaic, no duplicates)\n",
        "list_of_ids = ee.List.sequence(0, s1_stack_no_duplicates.size().subtract(1))\n",
        "# Turn s1_stack into a list\n",
        "stack_list = s1_stack_no_duplicates.toList(s1_stack_no_duplicates.size())\n",
        "\n",
        "# Function for adding the index value as the property ID (unique id# = system index#)\n",
        "def addID(index):\n",
        "  image = ee.Image(stack_list.get(index))\n",
        "  indexNum = ee.Number(index)\n",
        "  image = image.set('ID', indexNum)\n",
        "  return image\n",
        "s1_stack = ee.ImageCollection(list_of_ids.map(addID))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a RF Classifer"
      ],
      "metadata": {
        "id": "VRO3BHMl8aju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Land cover info is stored in the 'class' column, where 1 = water, 0 = land\n",
        "label = 'class'\n",
        "\n",
        "# Specify predictor data bands\n",
        "bands_of_interest = ['VV', 'VH', 'angle', 'VV_stdDev','VH_stdDev','VH_asm', 'VH_contrast', 'VH_corr', 'VH_idm']\n",
        "\n",
        "# Create a random forest classifier with 500 trees\n",
        "# Get raw probability from random forest instead of doing discrete classification\n",
        "# Source: https://gis.stackexchange.com/questions/447452/predict-classification-probability-of-pretrained-random-forest-in-gee\n",
        "ee_classifier = ee.Classifier.smileRandomForest(500).setOutputMode('RAW').train(TrainingSamples, label, bands_of_interest)"
      ],
      "metadata": {
        "id": "S_l0BOJ48kGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH-1VuKBYX3I"
      },
      "source": [
        "### Run Random Forest (500 trees) on the Image Collection and Aggregate the Images by Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQoLQIuQYRfF"
      },
      "outputs": [],
      "source": [
        "## Set up a function to run the rf classifier for every image in the image collection (interesting to do mean of trees and than do the mean of the means)\n",
        "def classification(image):\n",
        "  classified = image.select(bands_of_interest).classify(ee_classifier)\n",
        "  prob = classified.arrayReduce(ee.Reducer.mean(), ee.List([0])).arrayGet(0)\n",
        "  return prob\n",
        "\n",
        "# Apply the RF model to the image collection\n",
        "classified_result = s1_stack.map(classification)\n",
        "\n",
        "# Merge the probailities using mean (majority might work again)\n",
        "SAR_WaterProbability = classified_result.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Visualization (only run this if absolutely necessary)"
      ],
      "metadata": {
        "id": "5LKiu7NEFCMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the map function will consumes a lot of memory to run the previously submitted operations on GEE's server\n",
        "\n",
        "#Map.addLayer(SAR_WaterProbability, {'min':0, 'max':1, 'palette': ['964B00','236B8E']},'SAR Water Probability')\n",
        "#Map.centerObject(aoi, zoom = 12)\n",
        "#Map"
      ],
      "metadata": {
        "id": "T2wSr69FCkPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Export SAR Probability Raster to Google Drive and GEE Assets"
      ],
      "metadata": {
        "id": "wnZL8qZbIPbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive Folder Name?\n",
        "FolderName = 'HydroReservoir_Data'\n",
        "# GEE Asset Directory?\n",
        "Export_AssetDirectory = 'projects/ee-climatechangehydrology/assets/reservoirs_s1c_tiff/'\n",
        "\n",
        "# Export the raster to Google Drive\n",
        "Export_ProbailityRaster_SAR = ee.batch.Export.image.toDrive(image=SAR_WaterProbability,region = aoi_buff.geometry(),folder=FolderName, description= str(res_name)+'ProbabilityRaster_S1C', fileFormat='GeoTIFF',scale=10, maxPixels=1e13,skipEmptyTiles=True)\n",
        "Export_ProbailityRaster_SAR.start()\n",
        "\n",
        "# Export the raster as an GEE Asset\n",
        "Export_SAR_GEE = ee.batch.Export.image.toAsset(image=SAR_WaterProbability, region = aoi_buff.geometry(),description=str(res_name)+'_s1c', assetId=Export_AssetDirectory+str(res_name)+'_s1c', scale=10, maxPixels=1e13)\n",
        "Export_SAR_GEE.start()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w36uAYWqIO0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2KbDdJh0cvyp",
        "lHLY83gteASu",
        "VH-1VuKBYX3I"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}